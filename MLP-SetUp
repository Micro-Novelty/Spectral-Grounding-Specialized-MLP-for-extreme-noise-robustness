import numpy as np
import random
from sklearn.datasets import make_classification, make_moons, make_circles, load_breast_cancer, load_digits
from sklearn.model_selection import train_test_split

class Activation:
    @staticmethod
    def relu(x):
        return np.maximum(0, x)

    @staticmethod
    def relu_derivative(x):
        return (x > 0).astype(float)

    @staticmethod
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

    @staticmethod
    def sigmoid_derivative(x):
        s = Activation.sigmoid(x)
        return s * (1 - s)

    @staticmethod
    def softmax(x):
        # numerical stability
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

class Loss:
    @staticmethod
    def categorical_crossentropy(y_true, y_pred):
        eps = 1e-9
        y_pred = np.clip(y_pred, eps, 1 - eps)
        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))

    @staticmethod
    def softmax_crossentropy_derivative(y_true, y_pred):
        # IMPORTANT RESULT:
        # dL/dZ = y_pred - y_true
        return (y_pred - y_true) / y_true.shape[0]


class Dense:
    def __init__(self, x, input_size, output_size, activation=None, weight=None):
        self.special_weight = SpecialWeight(input_size, output_size)
        self.W = self.special_weight.weight_encoder(x)
        self.b = np.zeros((1, output_size))
        self.activation_name = activation

        if activation:
            self.activation = getattr(Activation, activation)
            self.activation_derivative = getattr(Activation, activation + "_derivative")
        else:
            self.activation = None
            self.activation_derivative = None


    def forward(self, x):
        self.x = x
        self.z = np.dot(x, self.W) + self.b

        if self.activation:
            self.a = self.activation(self.z)
        else:
            self.a = self.z

        return self.a


    def backward(self, da, lr):
        if self.activation_derivative:
            dz = da * self.activation_derivative(self.z)
        else:
            dz = da

        dW = np.dot(self.x.T, dz)
        db = np.sum(dz, axis=0, keepdims=True)
        dx = np.dot(dz, self.W.T)

        self.W -= lr * dW
        self.b -= lr * db

        return dx

class SoftmaxOutput:
    def forward(self, x):
        self.out = Activation.softmax(x)
        return self.out

    def backward(self, dL_dZ):
        # gradient already computed as y_pred - y_true
        return dL_dZ


class MLP:
    def __init__(self):
        self.layers = []
        self.softmax = SoftmaxOutput()

    def add(self, layer):
        self.layers.append(layer)

    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return self.softmax.forward(x)

    def backward(self, grad, lr):
        grad = self.softmax.backward(grad)
        for layer in reversed(self.layers):
            grad = layer.backward(grad, lr)

    def train(self, X, y, epochs=1000, lr=0.01, verbose=True):
        for epoch in range(epochs):
            y_pred = self.forward(X)
            loss = Loss.categorical_crossentropy(y, y_pred)
            grad = Loss.softmax_crossentropy_derivative(y, y_pred)
            self.backward(grad, lr)

            if verbose and epoch % 100 == 0:
                acc = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))
                print(f"Epoch {epoch} | Loss: {loss:.4f} | Acc: {acc:.2f}")



X, y_raw = make_moons(
    n_samples = 1000, 
    noise=0.75,  
    random_state=99)

X, y_raw = make_classification(
    n_samples=1000,
    n_features=3,
    n_classes=3,
    n_informative=3,
    n_redundant=0,
    class_sep=1.5,
    random_state=99
)

num_classes= len(np.unique(y_raw))
y = np.zeros((y_raw.size, num_classes))
y[np.arange(y_raw.size), y_raw] = 1

input_dim = X.shape[1]
output_dim = y.shape[1]



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=99)
model = MLP()
model.add(Dense(X_train, input_dim, 2000, activation="relu", weight="abs"))
model.add(Dense(X_train, 2000, output_dim, activation='relu', weight='abs'))  # logits


model.train(X_train, y_train, epochs=1000, lr=0.1)
def add_noise(X, noise_level=0.1):
    noise = np.random.normal(0, noise_level, X.shape)
    return X + noise

X_noisy = add_noise(X_test, noise_level=0.9)

def evaluate(model, x, y):
    pred = model.forward(x)
    pred_classes = np.argmax(pred, axis=1)
    true_classes = np.argmax(y, axis=1)
    accuracy = np.mean(pred_classes == true_classes)
    return accuracy



train_acc = evaluate(model, X_train, y_train)
test_acc = evaluate(model, X_test, y_test)
noisy_acc = evaluate(model, X_noisy, y_test)
print('Accuracy test', train_acc)
print('test accuracy', test_acc)
print('Noisy accuracy', noisy_acc)


